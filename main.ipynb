{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Device for gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Hyper parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "num_epochs = 5 # amount of times the model will see the data\n",
    "batch_size = 10 # amount of data to be processed at once\n",
    "learning_rate = 0.001 # gradient descent step size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform image Data\n",
    "# transforms the data into a tensor and normilizes it\n",
    "# transform = transforms.Compose(\n",
    "#             [transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
    "''' normalizes the data to be between -1 and 1 \n",
    "    Color images have three channels (red, green, blue),therefore you need three parameters to normalize\n",
    "    each channel.The first tuple (0.5, 0.5, 0.5) is the mean for all three channels \n",
    "    and the second (0.5, 0.5, 0.5) is the standard deviation for all three channels.\n",
    "    if data value is 0 then when normalized we have:\n",
    "    (0 - 0.5(mean)) / 0.5(std) = -1\n",
    "    if data value is 1 then when normalized we have:\n",
    "    (1 - 0.5) / 0.5 = 1\n",
    "'''\n",
    "# We could also turn the data into grayscale images by using:\n",
    "# this will turn the values between 0 and 1\n",
    "# 0 = black and 1 = white and in between is a shade of gray\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "            transforms.Grayscale(num_output_channels=1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='emotion_dataset/train', transform=transform)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = iter(train_data_loader)\n",
    "print(next(train_data_loader)[0].shape)\n",
    "# first number in the shape is the batch size\n",
    "# second number in the shape is the number of channels\n",
    "# third number in the shape is the height of the image\n",
    "# fourth number in the shape is the width of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3059, 0.2510, 0.2549,  ..., 0.8470, 0.8038, 0.8038],\n",
      "         [0.2353, 0.2549, 0.3725,  ..., 0.8234, 0.8391, 0.8274],\n",
      "         [0.2274, 0.2157, 0.2196,  ..., 0.8274, 0.8313, 0.8548],\n",
      "         ...,\n",
      "         [0.1216, 0.1333, 0.1647,  ..., 0.1961, 0.2510, 0.2823],\n",
      "         [0.1216, 0.1451, 0.1647,  ..., 0.2274, 0.2823, 0.2902],\n",
      "         [0.1490, 0.1490, 0.1686,  ..., 0.2431, 0.2862, 0.2666]]])\n"
     ]
    }
   ],
   "source": [
    "for data in train_data_loader:\n",
    "    print(data[0][0])\n",
    "    break\n",
    "# the train_data_loader has 2 elements, the first element is the image data and the second element is the labels\n",
    "# the image data is a tensor of shape (batch_size, channels, height, width)\n",
    "# the labels are a tensor of shape (batch_size)\n",
    "# if we do data[0][0] we are getting the first image in the batch\n",
    "# if we do data[0] we are getting the whole batch of images\n",
    "# if we do data[1] we are getting the whole batch of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
